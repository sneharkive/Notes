**1. Timeliness in Software:**
Delivering software **on time** to meet deadlines and user needs without delays.

**2. Portability in Software:**
The ability of software to **run on different platforms or environments** with minimal modification.

**3. R&D Level of Software Process:**
Refers to **research and development activities** for improving tools, methods, and technologies in software engineering.

**4. Iterative Waterfall & Incremental Approach:**

* **Iterative Waterfall:** Phases repeated until goals are met.
* **Incremental:** Software developed and delivered in **small functional parts** (increments).

**5. Incremental Approach:**
Builds software **step by step**, adding new features in each release.

**6. JAD (Joint Application Development) Approach:**
A method where **developers and users work together** in workshops to define requirements and design solutions.

**7. Activities During Project Initiation:**
Feasibility study, requirement gathering, defining scope, identifying stakeholders, preparing project charter.

**8. Issues Discussed During Project Closure:**
Final deliverables, documentation, lessons learned, client approval, team release.

**9. Drawbacks of RAD Model:**
Needs skilled team, not suitable for large/complex projects, poor documentation, high cost for small systems.

**10. Shortcomings of Classic Life Cycle (Waterfall):**
Rigid, no feedback, difficult to handle changes, late error detection, high risk.

**11. Task Regions in Spiral Model:**
Planning, risk analysis, engineering, evaluation.

**12. RAD vs Spiral Model:**

* **RAD:** Focus on speed, less risk analysis.
* **Spiral:** Focus on risk analysis and iterative refinement.

**13. Phases of Product Development Life Cycle:**
Requirement, design, implementation, testing, deployment, maintenance.

**14. Types of Prototyping:**
Throwaway, evolutionary, incremental, extreme.

**15. Pros–Cons of Linear Sequential Model:**
✅ Simple, structured.
❌ Inflexible, late testing, poor for changing requirements.

**16. Most Effective Paradigm:**
**Spiral model** — balances risk, flexibility, and iterative development.

**17. Selecting Life Cycle Model (by Team Features):**
Based on **team size, experience, and skills** — small skilled team → RAD; large structured → Waterfall.

**18. Selecting Life Cycle Model (by Project Type):**
Small/simple → Waterfall; large/complex → Spiral; evolving → Agile.

**19. Selecting Life Cycle Model (by User Community):**
When user feedback is critical → Prototyping or Agile models.

**20. OO (Object-Oriented) Life Cycle Model:**
Follows object-oriented principles: analysis, design, implementation, testing, maintenance using **objects and classes**.

**21. Evolutionary Development Model:**
System built and improved **through multiple versions**, based on feedback and evolving requirements.

**22. Iterative Enhancement Model:**
Software refined **in successive versions**, each improving functionality and performance.

**23. Build-and-Fix Model:**
Code written without formal design → test → fix → repeat; simple but **unreliable and unstructured**.

---

**24. Future Aspects of Software Engineering:**
AI-assisted coding, automation, quantum computing, model-driven and sustainable software.

**25. Applications in Daily Life:**
Used in banking, healthcare, transport, education, e-commerce, and communication systems.

**26. Role of Computer Science:**
Provides **algorithms, data structures, and theoretical foundations** essential for software engineering.

**27. Organization of SPMP (Software Project Management Plan):**
Includes project overview, organization, resources, schedule, risk management, and monitoring plan.

**28. Structured Programming (Example):**
Uses sequence, selection, and iteration (e.g., `if`, `while`) for clear, modular, and readable code.

**29. Software Design Decomposition Techniques:**
Functional decomposition, modularization, and object-oriented decomposition.

**30. General Design Process:**
Analysis → Architectural design → Interface design → Detailed design → Review.

**31. Benefits of Good SRS:**
Clear requirements, better communication, reduced rework, improved design and testing.

**32. Avoid Ambiguity in SRS:**
Use consistent terms, standard templates, clear diagrams, and precise language.

**33. Guidelines to Create Use Cases:**
Identify actors, define goals, describe main flow and alternate flows, ensure clarity and completeness.

**34. Difference Between Bugs and Defects:**
Bug – error found during testing;
Defect – issue found after release by users.

**35. Traditional vs Agile Testing:**
Traditional – sequential, documentation-heavy;
Agile – iterative, continuous feedback.

**36. Cyclomatic Complexity:**
Metric that measures **independent paths in code**, indicating program complexity.

**37. Process Metrics:**
Measure process performance — e.g., effort, productivity, defect removal rate.

**38. Product Metrics:**
Measure product quality — e.g., reliability, maintainability, size, performance.

**39. Stub Testing:**
Use of dummy modules to simulate missing components during top-down testing.

**40. Purpose of Test Plan:**
Defines **scope, objectives, resources, schedule**, and testing strategy.

**41. Manual Testing Types:**
Unit, integration, system, acceptance, regression.

**42. Test Log:**
Record of **test execution details**, including results, status, and issues found.

**43. Pros–Cons of Size Measurement:**
✅ Helps estimation and cost control.
❌ Difficult to measure early; may ignore complexity.

**44. Estimation Techniques:**
Expert judgment, Function Point, LOC, Use Case Points, Delphi method.

**45. Alpha vs Beta Testing:**

* **Alpha:** In-house testing by developers/testers.
* **Beta:** Field testing by real users.

**46. Test Data Suit:**
A set of **input data** prepared to validate software behavior and correctness.

**47. Features of Critical Module:**
High complexity, dependency, data sensitivity, and impact on system performance.

**48. Common Errors in Unit Testing:**
Incorrect logic, boundary errors, wrong data types, uninitialized variables.

**49. Black Box Testing Errors:**
Missing functions, incorrect output, interface mismatch, performance issues.

**50. Reasons for Software Testing:**
To **detect defects**, ensure quality, validate requirements, and improve reliability.

---

