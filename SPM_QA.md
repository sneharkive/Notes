### **1. What is timeliness in software?**

Timeliness refers to **delivering software within the planned schedule** while meeting user needs and quality standards. A timely product increases customer satisfaction and reduces project cost. Delays often lead to loss of business and credibility.

---

### **2. What is meant by portability in software?**

Portability means a software system‚Äôs **ability to operate on different hardware or OS environments** with minimal modification. Example: a Java application running on Windows, Linux, and macOS without code changes.

---

### **3. What is meant by R & D level of software process?**

R & D (Research & Development) level focuses on **innovating new methods, models, and tools** to improve software quality and productivity. It explores advanced technologies, e.g., AI-based coding assistants or new testing frameworks.

---

### **4. Define iterative waterfall model and incremental approach.**

* **Iterative Waterfall Model:** Extends the traditional waterfall by allowing **feedback between phases**; developers revisit earlier stages based on results from later ones.
* **Incremental Approach:** Software is built and delivered **in small increments**, each adding new functionality until the final system is complete.

---

### **5. Define incremental approach.**

An incremental approach divides the project into **manageable parts (increments)**. Each increment passes through design, implementation, and testing, producing a working version of the software that evolves over time.

---

### **6. What is meant by JAD approach?**

**Joint Application Development (JAD)** is a technique where **users, analysts, and developers collaborate** in structured workshops to gather requirements and make quick decisions. It improves communication and reduces rework.

---

### **7. List the activities during project initiation.**

1. Conduct feasibility study.
2. Identify stakeholders.
3. Define project scope and objectives.
4. Prepare project charter and initial plan.
5. Obtain management approval to proceed.

---

### **8. What are the issues discussed during project closure?**

* Verification of deliverables.
* Client acceptance and sign-off.
* Documentation and final reports.
* Lessons learned and performance review.
* Release of team members and resources.

---

### **9. What are the drawbacks of RAD model?**

1. Requires highly skilled team and tools.
2. Unsuitable for large, complex projects.
3. Limited scalability and poor documentation.
4. Dependence on continuous user availability.
5. Integration problems if prototypes differ.

---

### **10. Shortcomings of the classic life-cycle (waterfall) model**

* Rigid and sequential‚Äîno flexibility for change.
* Errors detected late in testing phase.
* Poor adaptability to evolving requirements.
* User feedback comes too late.
* High project risk and rework cost.

---

### **11. List all task regions in the Spiral Model.**

The **Spiral Model** includes four main task regions in each cycle (iteration):

1. **Planning:** Identify objectives, alternatives, and constraints.
2. **Risk Analysis:** Evaluate potential risks and develop mitigation strategies.
3. **Engineering:** Design, develop, and verify the next level of the product.
4. **Evaluation:** Review outcomes with stakeholders before starting the next cycle.
   üëâ Each loop results in a refined version of the system, combining design and prototyping.

---

### **12. Differentiate between RAD and Spiral Models.**

| **Feature**   | **RAD Model**                        | **Spiral Model**                       |
| ------------- | ------------------------------------ | -------------------------------------- |
| Focus         | Rapid application development        | Risk analysis and iterative refinement |
| Prototyping   | Uses multiple small prototypes       | Uses risk-driven prototyping           |
| Risk Handling | Limited                              | Explicitly analyzed                    |
| Team          | Requires highly skilled, small teams | Suitable for large, complex projects   |
| Time          | Fast delivery                        | May take longer but safer              |

---

### **13. Phases of Product Development Life Cycle (PDLC)**

1. **Requirement Analysis** ‚Äì Understand customer needs.
2. **Design** ‚Äì Architectural and detailed design of the system.
3. **Implementation** ‚Äì Coding and integration of modules.
4. **Testing** ‚Äì Verification and validation.
5. **Deployment** ‚Äì Delivery and installation of the system.
6. **Maintenance** ‚Äì Updating, fixing, and enhancing the product.

---

### **14. Different Types of Prototyping**

1. **Throwaway (Rapid):** Quickly built, discarded after requirements are clear.
2. **Evolutionary:** Continuously refined until the final system evolves.
3. **Incremental:** Developed in small functional parts.
4. **Extreme Prototyping:** Used in web applications (data ‚Üí interface ‚Üí services).

---

### **15. Pros and Cons of the Linear Sequential Model**

**Pros:**

* Simple, structured, and easy to manage.
* Suitable for projects with stable requirements.
* Milestone-based tracking is easy.

**Cons:**

* Poor flexibility for changes.
* Late feedback and testing.
* High risk of failure if requirements change mid-project.

---

### **16. Most Effective Software Engineering Paradigm (and Why)**

The **Spiral Model** is most effective because it:

* Incorporates **risk analysis** at every stage.
* Allows **iterative development** with user feedback.
* Is suitable for large, high-risk, or evolving projects.

---

### **17. Selecting a Life Cycle Model Based on Project Team Features**

* **Small, skilled team:** RAD or Agile ‚Äî fast delivery possible.
* **Large, structured team:** Waterfall or V-Model ‚Äî clear phase division.
* **Distributed team:** Iterative or Incremental ‚Äî parallel development possible.

---

### **18. Selecting a Life Cycle Model Based on Project Type**

* **Small/simple system:** Waterfall Model.
* **Complex, high-risk system:** Spiral Model.
* **Prototype-based application:** Prototyping Model.
* **Time-sensitive product:** RAD or Agile.

---

### **19. Selecting a Life Cycle Model Based on User Community**

* **Active user involvement:** Agile or Prototyping.
* **Limited user feedback:** Waterfall.
* **Users requiring frequent updates:** Incremental or Evolutionary Model.

---

### **20. Describe the Object-Oriented (OO) Life Cycle Model.**

The OO model integrates **object-oriented principles** into the development process.
**Phases:**

1. **OO Analysis:** Identify objects, classes, and relationships.
2. **OO Design:** Define class hierarchies, interfaces, and interactions.
3. **Implementation:** Code using OOP languages (Java, C++).
4. **Testing:** Unit and integration tests based on classes and interactions.
5. **Maintenance:** Enhancements and bug fixes.
   **Advantage:** Promotes reusability, modularity, and scalability.

---

### **21. Define Pilot Testing**

**Pilot Testing** is a **preliminary testing phase** where the system is deployed to a **limited number of users** before full rollout.

* Purpose: Detect issues in real environments.
* Ensures system performance, usability, and reliability.
* Acts as a rehearsal for actual implementation.

---

### **22. Define Meta-Model**

A **Meta-Model** is a **model of models** ‚Äî it defines the **rules, relationships, and structure** for creating specific models.
Example:

* UML meta-model defines how UML diagrams (like class or sequence diagrams) are constructed.

---

### **23. Win-Win Spiral Model**

An extension of the Spiral Model proposed by Boehm.

* Each stakeholder must achieve a **‚Äúwin‚Äù** ‚Äî meaning their objectives are satisfied.
  **Phases:**

1. **Identify Stakeholders and Objectives.**
2. **Resolve Conflicts and Negotiate Win-Win conditions.**
3. **Develop and Validate the Product.**
4. **Review and Plan Next Iteration.**

**Goal:** Balance customer satisfaction, developer constraints, and business goals.

---

### **24. Software Project Manager**

A **Software Project Manager (SPM)** plans, organizes, monitors, and controls software projects.
**Main Roles:**

* Define scope and objectives.
* Manage budget and time.
* Assign tasks and monitor team progress.
* Handle risks and communication.

**Skills:** Technical knowledge, leadership, communication, risk management.

---

### **25. Software Design Model**

A **Software Design Model** is a **representation of the software structure** showing how the system will satisfy requirements.
**Types:**

1. **Architectural Design:** Overall structure (modules, data flow).
2. **Detailed Design:** Internal logic of modules.
3. **Interface Design:** Defines interaction between modules and users.
   **Goal:** Transform SRS into a blueprint for coding.

---

### **26. Interface Design**

Interface design defines **how software components, systems, and users interact**.
**Types:**

* **User Interface (UI):** Layout, screens, commands.
* **System Interface:** Communication between software components.
  **Principles:**
* Simplicity, consistency, feedback, and user control.

---

### **27. SCM (Software Configuration Management)**

SCM is the process of **controlling and tracking software changes** to maintain integrity.
**Functions:**

1. **Version Control:** Track code versions.
2. **Change Control:** Manage modifications.
3. **Configuration Identification:** Label software components.
4. **Status Accounting:** Record and report changes.
5. **Auditing:** Verify that changes were properly implemented.

---

### **28. Software Reliability, Availability, Maintainability**

* **Reliability:** Probability that software performs correctly for a given period.
* **Availability:** Fraction of time the system is operational and accessible.
  [
  \text{Availability} = \frac{\text{MTBF}}{\text{MTBF} + \text{MTTR}}
  ]
* **Maintainability:** Ease with which software can be modified or repaired.

---

### **29. Defect**

A **defect** is an **imperfection or flaw** in software that causes it to produce incorrect or unexpected results.
**Example:** Wrong algorithm, incorrect logic, missing validation.
**Detected During:** Testing, code review, or user operation.

---

### **30. Reliability ‚Äî Metrics, Tools, Price**

**Metrics:**

* **MTTF (Mean Time To Failure)**
* **MTTR (Mean Time To Repair)**
* **MTBF (Mean Time Between Failures)**
* **Failure Rate**

**Tools:**

* **JMeter, LoadRunner, Bugzilla,** and **ReliSoft** for reliability testing.

**Price (Cost of Reliability):**

* Involves cost of fault prevention, fault detection, and correction.
* High reliability ‚Üí increased testing and maintenance cost.

---

### **31. Alpha Testing and Beta Testing**

**Alpha Testing:**

* Performed **in-house** by developers or testers before product release.
* Conducted in a **controlled environment**.
* Purpose: Detect defects early before public use.
* Example: Testing an app internally by QA team.

**Beta Testing:**

* Conducted by **real users in real environments**.
* Helps gather feedback and find issues missed in alpha testing.
* Example: Google releasing a beta version of Android to selected users.

---

### **32. Standard of Software Product**

Software standards define **quality benchmarks and best practices** for consistent, reliable development.
**Examples:**

* **ISO/IEC 9126:** Defines software quality characteristics (functionality, reliability, usability, efficiency, maintainability, portability).
* **IEEE 830:** SRS standard format.
  **Importance:** Ensures quality, interoperability, and maintainability.

---

### **33. Six Sigma**

A **quality improvement methodology** that aims to reduce defects and variations in processes.

* Originated at Motorola; uses statistical analysis.
* **Goal:** 3.4 defects per million opportunities (near perfection).
  **Phases (DMAIC):**

1. Define
2. Measure
3. Analyze
4. Improve
5. Control
   **Benefits:** Increased quality, customer satisfaction, and process efficiency.

---

### **34. Pattern**

A **Pattern** is a **general reusable solution** to a commonly occurring design problem.
**Types:**

1. **Creational Patterns:** Object creation (e.g., Singleton, Factory).
2. **Structural Patterns:** Class and object composition (e.g., Adapter, Composite).
3. **Behavioral Patterns:** Communication between objects (e.g., Observer, Strategy).
   **Advantage:** Reuse proven solutions, improve maintainability.

---

### **35. Structured Analysis Tools**

Used in system analysis to model data flow and structure.
**Common Tools:**

1. **DFD (Data Flow Diagram):** Shows data movement in a system.
2. **ER Diagram:** Represents data entities and relationships.
3. **Structure Chart:** Shows module hierarchy.
4. **Decision Tables:** Define complex logic systematically.

**Purpose:** Simplifies understanding and communication of system functionality.

---

### **36. Negative and Positive Test Cases**

* **Positive Test Case:** Checks software with **valid inputs** to ensure correct output.
  Example: Enter correct username and password ‚Üí login succeeds.
* **Negative Test Case:** Uses **invalid inputs** to test system robustness.
  Example: Enter blank password ‚Üí system should show error message.
  **Purpose:** Ensures software behaves correctly in both expected and unexpected conditions.

---

### **37. Fault vs Failure vs Error**

| **Term**        | **Definition**                                | **Example**                  |
| --------------- | --------------------------------------------- | ---------------------------- |
| **Error**       | Human mistake during coding/design            | Typo in code                 |
| **Fault (Bug)** | Defect in the program code                    | Wrong logic in a function    |
| **Failure**     | Deviation of system behavior during execution | Program crashes during login |

**Relation:** Error ‚Üí introduces Fault ‚Üí leads to Failure.

---

### **38. Stub**

A **stub** is a **dummy module** used during **top-down integration testing** to simulate missing components.

* Accepts inputs and returns fixed outputs.
* Helps test higher-level modules before lower ones are ready.
  **Example:** Stub for a payment gateway returning ‚ÄúSuccess‚Äù before actual implementation.

---

### **39. Modular Design**

Divides software into **independent, reusable modules** that can be developed and tested separately.
**Advantages:**

* Easier maintenance and debugging.
* Promotes code reuse.
* Reduces complexity by isolating functionality.
  **Example:** Separate modules for authentication, database, and UI in a web app.

---

### **40. Data Dictionary**

A **centralized repository** that stores **definitions, structures, and attributes of all data elements** used in a system.
**Contents:**

* Data names, types, formats, and size.
* Relationships among data items.
* Source and destination of data.
  **Purpose:** Ensures consistency, improves communication among developers, and supports documentation.

---

### **41. Verification and Validation**

**Verification:**

* Ensures software meets **design specifications**.
* *‚ÄúAre we building the product right?‚Äù*
* Activities: Reviews, inspections, walkthroughs.

**Validation:**

* Ensures software meets **user requirements**.
* *‚ÄúAre we building the right product?‚Äù*
* Activities: Testing, user acceptance.

---

### **42. Black Box vs White Box Testing**

| **Aspect**           | **Black Box Testing**                  | **White Box Testing**        |
| -------------------- | -------------------------------------- | ---------------------------- |
| **Focus**            | Functional behavior                    | Internal logic/code          |
| **Knowledge Needed** | No code knowledge                      | Requires code knowledge      |
| **Performed By**     | Testers                                | Developers                   |
| **Examples**         | Functional, System, Acceptance testing | Unit testing, Branch testing |

---

### **43. Integration Testing**

Integration testing checks the **interaction between integrated modules**.
**Purpose:** To detect interface defects and data flow issues.
**Types:**

* **Top-Down:** Uses *stubs*.
* **Bottom-Up:** Uses *drivers*.
* **Sandwich (Hybrid):** Combines both approaches.

---

### **44. System Testing**

System testing validates the **complete integrated system** to ensure it meets requirements.
**Types:** Functional, Performance, Security, and Stress testing.
**Goal:** Ensure the system works as a whole before delivery.

---

### **45. Acceptance Testing**

Performed by the **customer or end-user** to verify that the system satisfies business needs.
**Types:**

* **Alpha Testing:** At developer site.
* **Beta Testing:** At customer site.
  **Purpose:** Final approval before deployment.

---

### **46. Test Planning**

A **Test Plan** is a formal document that outlines **testing scope, objectives, approach, schedule, and resources**.
**Contents:**

* Test objectives, strategy, schedule, environment, deliverables, exit criteria.
  **Goal:** Ensure systematic and effective testing process.

---

### **47. Software Metrics**

Quantitative measures that help assess software **quality, productivity, and performance**.
**Types:**

* **Product Metrics:** Size, complexity, reliability (e.g., LOC, Cyclomatic Complexity).
* **Process Metrics:** Efficiency, defect removal rate.
* **Project Metrics:** Cost, effort, and time estimation.

**Purpose:** Improve control and decision-making in software projects.

---

### **48. Estimation Techniques**

Used to predict **effort, time, and cost** of software projects.
**Common Methods:**

1. **Expert Judgment:** Based on prior experience.
2. **Delphi Technique:** Consensus among experts.
3. **COCOMO Model:** Uses equations based on size (KLOC).
4. **Function Point Analysis:** Based on user functions and data.

**Goal:** Achieve accurate planning and resource allocation.

---

### **49. White Box Testing Techniques**

Used to test **internal structure or logic** of code.
**Techniques:**

1. **Statement Coverage:** Every statement executed at least once.
2. **Branch Coverage:** All decision outcomes tested.
3. **Path Coverage:** All possible execution paths tested.
4. **Loop Testing:** Tests loops for boundary conditions.

---

### **50. Software Testing Life Cycle (STLC)**

A structured process defining **all stages of testing**.
**Phases:**

1. **Requirement Analysis** ‚Äì Understand testable requirements.
2. **Test Planning** ‚Äì Prepare strategy and schedule.
3. **Test Case Design** ‚Äì Write test cases and scenarios.
4. **Environment Setup** ‚Äì Prepare test environment.
5. **Test Execution** ‚Äì Run test cases.
6. **Defect Tracking** ‚Äì Log and fix bugs.
7. **Test Closure** ‚Äì Document results and lessons learned.

---

### **51. Pros and Cons of Big-Bang Testing**

**Pros:**

* Simple; no planning required.
* All modules integrated and tested together.
* Good for small systems.

**Cons:**

* Difficult to isolate defects.
* Late error detection.
* Poor for large, complex systems.
* High debugging cost and time.

---

### **52. OO (Object-Oriented) Testing**

Testing focused on **objects and classes** in OOP systems.
**Levels:**

1. **Unit testing:** Tests class methods.
2. **Integration testing:** Tests interactions between objects.
3. **System testing:** Verifies complete OO system.
   **Features:** Inheritance, polymorphism, encapsulation are major test focuses.

---

### **53. Software Science Metrics**

Proposed by **Halstead** ‚Äî measures software complexity using operators and operands.
**Metrics include:**

* Program Length (N) = N‚ÇÅ + N‚ÇÇ
* Vocabulary (n) = n‚ÇÅ + n‚ÇÇ
* Volume (V) = N √ó log‚ÇÇ(n)
  Used to estimate development effort, cost, and maintainability.

---

### **54. Phases of Detailed COCOMO Model**

1. **Application Composition:** Early prototyping.
2. **Early Design:** Rough design; cost by function points.
3. **Post-Architecture:** Detailed design; cost by LOC and factors.
   Each phase refines estimates of effort, schedule, and cost.

---

### **55. Cost Drivers in Intermediate COCOMO**

Key drivers affecting cost:

* Product attributes: reliability, complexity.
* Computer attributes: memory, runtime constraints.
* Personnel attributes: experience, capability.
* Project attributes: tools, schedule.

---

### **56. Different Software Cost Components**

1. **Hardware and Software Costs**
2. **Manpower Costs (Development & Maintenance)**
3. **Training Costs**
4. **Quality Assurance and Testing Costs**
5. **Overheads (management, communication)**

---

### **57. Software Construction Tools**

* **Compilers & Interpreters**
* **Debuggers**
* **Code Generators**
* **Version Control Tools (Git)**
* **Build Automation Tools (Maven, Gradle)**

---

### **58. CASE Environment**

**CASE (Computer-Aided Software Engineering)** provides automated support for software development.
Includes tools for analysis, design, coding, testing, and documentation within an integrated framework.

---

### **59. Benefits of CASE Tools**

* Improves productivity and accuracy.
* Ensures documentation and standardization.
* Reduces development time and cost.
* Enhances maintenance and quality assurance.

---

### **60. Quality of Conformance**

The **degree to which software meets its design specifications** and quality requirements.
High conformance = fewer defects and better reliability.

---

### **61. FTR (Formal Technical Review)**

A structured peer review to detect errors early.
**Objectives:**

* Ensure requirements/design correctness.
* Verify standards compliance.
* Improve product quality and team communication.

---

### **62. Uses of Process Technology Tools**

* Automate repetitive tasks.
* Improve project monitoring.
* Maintain process consistency.
* Support metrics collection and reporting.

---

### **63. Principles of Software Project Scheduling**

1. Define tasks clearly.
2. Estimate time accurately.
3. Identify task dependencies.
4. Allocate resources effectively.
5. Monitor and adjust progress continuously.

---

### **64. Aspects of Software Reuse**

* **Code reuse:** Reusing tested modules.
* **Design reuse:** Reusing architectures or patterns.
* **Test reuse:** Using previous test cases.
  **Benefits:** Reduces cost, time, and improves quality.

---

### **65. Categories of Software Risk**

1. **Project Risk:** Schedule, budget, personnel.
2. **Technical Risk:** Technology failure, design issues.
3. **Business Risk:** Market changes, cost overrun.
4. **Operational Risk:** Deployment or maintenance issues.

---

### **66. Risk Uncertainty Spectrum**

Graph showing **risk level vs. knowledge**:

* High uncertainty at early stages.
* Decreases as project progresses and data improves.

---

### **67. Objectives of SRM (Software Risk Management)**

* Identify potential risks early.
* Assess and prioritize risks.
* Develop mitigation and contingency plans.
* Monitor and control risk impacts throughout the project.

---

### **68. Benefits of Risk Management**

* Prevents project failure.
* Reduces losses and rework.
* Increases confidence in project success.
* Ensures resource optimization and better decision-making.

---

### **69. Major Rewards of Reliable Software**

* User satisfaction and trust.
* Lower maintenance costs.
* Higher availability and performance.
* Competitive market advantage.

---

### **70. Classification of Software Failure**

1. **Transient Failure:** Occurs occasionally.
2. **Permanent Failure:** Persistent malfunction.
3. **Recoverable Failure:** Can be corrected automatically.
4. **Non-recoverable Failure:** Requires manual correction.

---

### **71. Criteria to Choose a Reliability Model**

* Availability of historical failure data.
* Type of application (real-time, safety-critical).
* System complexity.
* Project duration and cost limits.

---

### **72. Tree Diagram for Risk Management Activities**

**Root:** Risk Management
‚Üí Identification
‚Üí Analysis (Qualitative & Quantitative)
‚Üí Evaluation
‚Üí Mitigation/Control
‚Üí Monitoring & Review

---

### **73. Main Activities in Risk Analysis Phase**

1. **Risk Identification:** List potential threats.
2. **Risk Assessment:** Evaluate likelihood and impact.
3. **Risk Prioritization:** Rank by severity.
4. **Risk Mitigation Planning:** Define responses.
5. **Monitoring:** Track and update during project life cycle.

---
